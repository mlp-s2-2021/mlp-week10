{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Machine Learning in Python - Workshop 10\n\nIn this week's workshop we will be exploring several methods for unsupervised learning, specifically some of sklearn's tools for clustering data. ",
      "metadata": {
        "cell_id": "00000-a2d2db23-4657-4eda-b822-2f55766117ff",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# 1. Setup\n\n\n## 1.1 Packages\n",
      "metadata": {
        "cell_id": "00001-4188bd78-acc2-4628-ae04-85421e9966e5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "In the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution. ",
      "metadata": {
        "cell_id": "00002-70e6e7b1-2d80-4d69-9956-78b307e3dc83",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-f4727649-c091-4cdf-b16d-ab893f9bea08",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "dc4ffc84",
        "execution_millis": 2,
        "execution_start": 1616599576273,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "# Display plots inline\n%matplotlib inline\n\n# Data libraries\nimport pandas as pd\nimport numpy as np\n\n# Plotting libraries\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ipywidgets \nfrom ipywidgets import interact\n\n# Plotting defaults\nplt.rcParams['figure.figsize'] = (10,6)\nplt.rcParams['figure.dpi'] = 80\n\n# sklearn modules\nimport sklearn\nimport sklearn.cluster\nimport sklearn.manifold\nimport sklearn.preprocessing\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.cluster.hierarchy import dendrogram\nimport scipy.cluster",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 1.2 Helper Functions\n\nFor this lab we will make use of a the `plot_dendrogram` function provided in sklearn's clustering documentation. This function takes an `AgglomerativeClustering` model and plots the hierarchical merging of clusters as a dendrogram, more on this later.",
      "metadata": {
        "cell_id": "00004-9c50c6e5-3993-4f73-8146-e514c3d26ff9",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00005-09854883-68fe-421f-b97d-4a29dda4df39",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e8d93e35",
        "execution_millis": 0,
        "execution_start": 1616599576294,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def get_labels(m):\n    if hasattr(m, 'labels_'):\n        return(m.labels_)\n    else:\n        return(m.steps[-1][1].labels_)\n    \n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n    # from sklearn documentaion:\n    # https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n    \n    # create the counts of samples under each node\n    \n    if (type(model) == sklearn.pipeline.Pipeline):\n        model = model.steps[-1][1]\n    \n    counts = np.zeros(model.children_.shape[0])\n    labels = get_labels(model)\n    n_samples = len(labels)\n    \n    \n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    Z = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    palette = [mpl.colors.rgb2hex(c) for c in sns.color_palette(n_colors = model.n_clusters_)]\n    \n    link_cols = {}\n    for i, i12 in enumerate(Z[:,:2].astype(int)):\n        c1, c2 = (link_cols[x] if x > len(Z) else palette[labels[x]] for x in i12)\n        link_cols[i+1+len(Z)] = c1 if c1 == c2 else \"#808080\"\n    \n    # Plot the corresponding dendrogram\n    dendrogram(Z, link_color_func=lambda x: link_cols[x], **kwargs)\n    \n    \n    if (model.n_clusters_ == 1):\n        threshold = np.max(model.distances_)\n    elif (model.distance_threshold is None):\n        i = model.n_clusters_-1\n        threshold = (model.distances_[-i] + model.distances_[-(i+1)])/2\n    else:\n        threshold = model.distance_threshold\n    \n    plt.axhline(y=threshold, color=\"0.25\", linestyle=\"--\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 1.3 Data\n\nWe will look at several toy data examples to start to get a better understanding of sklearn's clustering algorithms.",
      "metadata": {
        "cell_id": "00006-525ee576-9a9b-4d18-8bd5-a43e37291c72",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00007-da4a0d39-b148-4b2c-90c2-7058a39fcdf9",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9f775af",
        "execution_millis": 8,
        "execution_start": 1616599576295,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "data = {\n    \"d1\": pd.read_csv(\"d1.csv\"),\n    \"d2\": pd.read_csv(\"d2.csv\"),\n    \"d3\": pd.read_csv(\"d3.csv\"),\n    \"d4\": pd.read_csv(\"d4.csv\"),\n    \"d5\": pd.read_csv(\"d5.csv\"),\n    \"d6\": pd.read_csv(\"d6.csv\"),\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "In all six of the data sets we have observations in 2 dimensions with 1, 2, or 3 classes. Our goal will be to attempt to recover these labels as best we are able without providing the true labels to the underlying algorithm fitting our model(s), which is why these methods are called as unsupervised. ",
      "metadata": {
        "cell_id": "00008-4448dc30-aaab-493d-822c-baf311eeeea2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00009-af3bf763-b8ec-43cc-b0dd-b754e179bc6d",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "177fc49b",
        "execution_millis": 901,
        "execution_start": 1616599576322,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "plt.figure(figsize=(10.5,7))\n\nfor i, d in zip(range(len(data)), data):\n    plt.subplot(231+i)\n    ax = sns.scatterplot(x='x', y='y', hue='label', data=data[d], legend=False)\n    ax.set_title(d)\n    if (i < 3): ax.set_xlabel(\"\")\n    if (i % 3 != 0): ax.set_ylabel(\"\")\n    \nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n# 2. K-Means\n\nWe will begin by fitting k-means models to the data sets while adjusting several of the key model parameters. k-means models are straight forward to fit and use a the same sklearn interface that we are used to, they only differ in that they only require a model / data matrix `X` and do not require or use the outcome vector `y`. For the sake of compatibility with the supervised models in sklearn the `fit` method of these models still has an argument `y`, but this is argument is neither required nor used. \n\nSince we are often only interested in the result of the clustering, and not the underlying model, k-means and the other clustering models offer a `fit_predict` method which both fits the model and then predicts cluster labels as a single step for a data matrix `X`. This is similar in spirit to the `fit_transform` method that is present with the preprocessing transformer like `StandardScaler` and `OneHotEncoder`.\n\nBelow we implement a function for exploring all six data sets with the k-means algorithm. Try changing the number of clusters and the method for initializing the cluster centroids (for more on the k-means++ method see this [article](https://en.wikipedia.org/wiki/K-means%2B%2B#Improved_initialization_algorithm)) and the random state value to seed the random number generator before fitting the model.",
      "metadata": {
        "cell_id": "00010-dc9562a3-8958-4bc5-b1b0-7ff073e02127",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00011-fcc3a573-354c-41fd-b8d1-07f0088495a0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6542a0dd",
        "execution_millis": 0,
        "execution_start": 1616599596748,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def fit_kmeans(d='d1', n_clusters=2, init='random', random_state='1234'):\n    # Convert from string to int\n    random_state = int(random_state)\n    n_clusters = int(n_clusters)\n    \n    df = data[d]\n    X = df.drop(['label'], axis=1)\n\n    # Fit and predict\n    df['cluster'] = sklearn.cluster.KMeans(\n        n_clusters=n_clusters, init=init, random_state=random_state\n    ).fit_predict(X)\n    df = df.astype({'cluster': 'category'})\n    \n    # Plot\n    plt.figure(figsize=(7,7))\n    ax = sns.scatterplot(x='x', y='y', hue='cluster', style='label', data=df)\n    ax.set_title(d)\n    \n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 1\n\nFor data set `d1` what happens when you change the value of `random_state` with `n_clusters=2`? *Hint* - keep trying until something interesting happens.",
      "metadata": {
        "cell_id": "00012-fb72919a-3f9d-4efa-94a0-5dcf3bfc983d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "fit_kmeans('d1', n_clusters=2, init='random', random_state=1234)",
      "metadata": {
        "tags": [],
        "cell_id": "00013-95ecf919-c13f-48c2-a982-ab5f13b0bddf",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7e080721",
        "execution_millis": 325,
        "execution_start": 1616599626467,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00013-328964ee-dce3-436c-bc47-304123b4b3f8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 2\n\nFor data set `d1` set `n_clusters=3` do the clusters change as you vary `init` and `random_state`?",
      "metadata": {
        "cell_id": "00014-43ec044c-f821-486d-b9ba-2b39ff10e7dc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "fit_kmeans('d1', n_clusters=3, init='random', random_state=1234)",
      "metadata": {
        "tags": [],
        "cell_id": "00016-8d3ddd29-856b-4a41-8986-90af16d1b107",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "64e72922",
        "execution_start": 1616599641312,
        "execution_millis": 388,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00015-beef948b-8778-4d26-85d4-6bdf8bbfbe0a",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 3\n\nExplore the different data sets with different values of `n_cluster`, describe the nature of the boundaries produced by a k-means clustering. *Hint* - don't just consider the binary (`k=2`) case.",
      "metadata": {
        "cell_id": "00016-e045b209-543b-472b-b4cc-cb2f3925ab36",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00019-c004fc30-058b-4ed1-ab52-2f8700ee5bf8",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00017-cd9df802-5c33-44d7-98e8-fbac930b4180",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 4\n\nBased on what we have just seen and what you have learned from the lectures, explain why we can or cannot construct a confusion matrix using the predictions from the k-means model (for the purposes of model scoring).",
      "metadata": {
        "cell_id": "00018-94feb757-2632-484b-b344-e979fe9c3d03",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00019-7e628b8a-2c65-4dee-a364-632c59e79635",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\nIn all of the provided data sets the true number of clusters is provided and is also fairly obvious by visual inspection of the data. However, with real world data this value is unlikely to be known ahead of time. A simple way of potentially determining this value is to fit a series of k-means models with different values of `k` an then comparing the resulting within-cluster sum-of-squares (or what sklearn calls the inertia).\n",
      "metadata": {
        "cell_id": "00020-116ea7d7-8196-4c1b-a310-0dcb9e6b06de",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 5\n\nIn general, would you expect the within-cluster sum-of-squares (inertia) to increase, decrease or stay the same as `k` is increased?",
      "metadata": {
        "cell_id": "00021-e416b058-3ee1-40e1-a984-df4fa5aecfa1",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00022-30c04ee2-1058-4a63-9071-a712558eab2a",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\nBelow we have reused the code from our previous function but now we include a second plot that shows the inertia as a function of `n_cluster`. For each of the data sets play with different values of `n_clusters` and try to develop some intuition on how the inertia relates to the model's fit.",
      "metadata": {
        "cell_id": "00023-81e827fd-b6e9-4e3c-8bec-e8bc61c326c6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00024-0072c940-9c07-476a-b9bf-4616b76f1072",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7a07eccf",
        "execution_millis": 1,
        "execution_start": 1616599770154,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def fit_kmeans_inertia(d = 'd1', n_clusters=2):\n    # Read the data, create the model matrix, and fit and predict\n    df = data[d]\n    X = df.drop(['label'], axis=1)\n    \n    models = [ sklearn.cluster.KMeans(n_clusters=n).fit(X) for n in range(1,6)]\n    scores = [m.inertia_ for m in models]\n    \n    df['cluster'] = models[n_clusters-1].predict(X)\n    df = df.astype({'cluster': 'category'})\n    \n    # Plot\n    plt.figure(figsize=(12,6))\n    \n    plt.subplot(121)\n    sns.scatterplot(x='x', y='y', hue='cluster', style='label', data=df)\n    \n    plt.subplot(122)\n    plt.plot(np.arange(1,6), scores, 'o-')\n    plt.xlabel(\"n_clusters\")\n    plt.ylabel(\"inertia\")\n    plt.xticks(range(1,6))\n    \n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "fit_kmeans_inertia('d1', n_clusters=2)",
      "metadata": {
        "tags": [],
        "cell_id": "00028-2e893c97-ea0f-4966-b057-97b5ee6e4012",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5978efd7",
        "execution_start": 1616599778822,
        "execution_millis": 675,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 6\n\nBased on these plots, for which data sets does k-means appear to be doing a good job - i.e. it fits well and the correct number of clusters can be inferred from the data. For which data sets does it not do a good job?",
      "metadata": {
        "cell_id": "00025-a3451946-bd50-418d-954e-c3b2d248f5cb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00026-62f7360f-ad04-4bfd-b02a-85c9fea7bad3",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 7\n\nDescribe the features of the inertia plots that suggest whether a k-means model is fitting well and how this relates to determining $n_{clusters}$.",
      "metadata": {
        "cell_id": "00027-a40ed7d5-bd73-4f2c-bad9-5c2faf686b4e",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00028-e598f04e-8fa7-4139-8907-c1d8cde1e8d9",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# 3. Hierarchical Clustering\n\nWhile there is a large number of different methods for hierarchical clustering, the methods implemented in sklearn under the `AgglomerativeClustering` model use a bottom up approach by starting each observation as its own cluster and then merging existing clusters to create new larger clusters. The rule(s) used for merging these clusters are determined by the `linkage` argument which takes one of the following values:\n\n* `\"ward\"` - minimizes the sum of squared differences within all clusters.\n* `\"complete\"` - minimizes the maximum distance between observations of pairs of clusters.\n* `\"average\"` - minimizes the average of the distances between all observations of pairs of clusters.\n* `\"single\"` - minimizes the distance between the closest observations of pairs of clusters.\n\nBelow we define a function that will let you play with these different approaches. The function also provides the option of specifying either `n_clusters` or `distance_threshold` - the former is ignored if the latter is not `None` or `\"\"`.",
      "metadata": {
        "cell_id": "00029-22d7b2a3-7c82-430c-b327-aa1535ff17c2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00030-4112882e-f363-41fb-a837-fe0fa414e5db",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "53ceb039",
        "execution_millis": 2,
        "execution_start": 1616599880661,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def fit_aggclust(d='d1', n_clusters=2, distance_threshold=None, linkage='ward'):\n    #distance_threshold = float(distance_threshold)\n    \n    try:\n        distance_threshold = float(distance_threshold)\n    except:\n        distance_threshold = None\n    \n    if (distance_threshold is not None):\n        n_clusters = None\n    \n    d = data[d]\n    X = d.drop(['label'], axis=1)\n    \n    # Fit model\n    m = sklearn.cluster.AgglomerativeClustering(\n        distance_threshold=distance_threshold, n_clusters=n_clusters, \n        linkage=linkage, compute_full_tree = True, compute_distances = True\n    ).fit(X)\n    \n    d['cluster'] = m.labels_\n    d['cluster'] = d['cluster'].astype('category')\n    \n    # Plot\n    plt.figure(figsize=(12,6))\n    \n    plt.subplot(121)\n    sns.scatterplot(x='x', y='y', hue='cluster', style='label', data=d)\n    \n    plt.subplot(122)\n    plot_dendrogram(m, show_leaf_counts=False, no_labels=True)\n\n    \n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "fit_aggclust('d1', n_clusters=3, distance_threshold=None, linkage='ward')",
      "metadata": {
        "tags": [],
        "cell_id": "00035-e05276da-c24b-466f-9e45-b85c1af4cbd1",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "99ad6ba7",
        "execution_start": 1616599913503,
        "execution_millis": 563,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "fit_aggclust('d1', n_clusters=3, distance_threshold=10, linkage='ward')",
      "metadata": {
        "tags": [],
        "cell_id": "00036-e4c0d7f5-c9eb-451f-b58f-7c551f603210",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "333c6fb6",
        "execution_start": 1616599932672,
        "execution_millis": 511,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 8\n\nHow does the `distance_threshold` argument relate to the number of clusters predicted by the model?",
      "metadata": {
        "cell_id": "00031-1b84bef5-2e71-4876-9545-de5aa87ffc68",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00032-4588ff0c-d491-4ba9-b8d1-da6686728ea9",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 9\n\nFor `d1` and `d2` which of the linkage method(s) work best for identifying the correct number of clusters present?",
      "metadata": {
        "cell_id": "00033-9a8cf18b-a0f9-4f72-abaf-1e30906194c3",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00040-30c6e534-1149-48f0-a624-c185a067e412",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00034-2ad84f6c-8f3e-43ef-953b-f678ec3b48ab",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 10\n\nWhat about for `d3` and `d4`? Which of the linkage method(s) work best for identifying the correct number of clusters present?",
      "metadata": {
        "cell_id": "00035-0b41ff7d-ee49-4f48-a34d-333f0612e406",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00042-5d13def5-8d07-4bb4-b577-a17404c97660",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00036-cdab04b0-2c69-4465-827e-7d1aa9a5d8f6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 11\n\nFinally, what about for `d5` and `d6`?",
      "metadata": {
        "cell_id": "00037-3ff9c02f-0cbd-409c-99ff-8d56019f8da1",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00044-9757fc1a-6497-430b-b6c7-6469c0dbb7e2",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00038-c35488e5-1d21-4de8-b89a-c23d82a4fe61",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 12\n\nBased on what you found with Exercises 9-11 can you come up with a reasonable heuristic for choosing which linkage function to use?",
      "metadata": {
        "cell_id": "00039-00bd82ae-e016-4dcb-9f04-794688cccb58",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00040-5c838b7e-175d-46a0-b982-f91ee4ab3368",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n# 4. Clustering Animals\n\nWe will wrap this workshop up by looking at a slightly more complicated example. These data come from the `palmerpenguin` R [package](https://github.com/allisonhorst/palmerpenguins) by Allison Horst and represent data collected on penguin species occuring around Palmer Station in Antarctica by Dr. Kristen Gorman. The data contain various measurement details of the sample penguin's physical characteristics as well as the specific island they were located on. The data provided in `palmerpenguin.csv` reflect a lightly modified version of the data available in the package (missing values and the `year` variable have been removed.)",
      "metadata": {
        "cell_id": "00042-7644797c-53f0-4798-8924-8e9bd21a94c5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00043-7a00b226-4b03-4202-b1c9-bf4a7919e51d",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5f3dc444",
        "execution_millis": 58,
        "execution_start": 1616599578753,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "penguin = pd.read_csv(\"palmerpenguins.csv\")\npenguin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Our goal is to now use the clustering methods we've just learned and see how well we are able to distinguish between the different species of penguin using an *unsupervised* modelling approach. Specifically, in this case we already know the species, but it is conceivable that a similar project could have been undertaken where the question of interest is how many species are present amoung the sampled individuals.\n\nBelow we set up a dictionary containing two pipelines, one for agglomerative clustering and the other for k means clustering. These pipelines are configured so that they take care of the dummy coding for the categorical variables and the basic configuration of the clustering models.",
      "metadata": {
        "cell_id": "00044-ad37c26d-b962-488a-87d1-85c642b2ddec",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00045-44f940e7-044f-4335-adc1-b0133a3f01cd",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f3451c84",
        "execution_millis": 1,
        "execution_start": 1616599578795,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "models = {\n    \"AgglomerativeClustering\": make_pipeline(\n        make_column_transformer(\n            (sklearn.preprocessing.OneHotEncoder(), ['island', 'sex']),\n            remainder = \"passthrough\"\n        ),\n        sklearn.cluster.AgglomerativeClustering(\n            compute_full_tree = True, compute_distances = True\n        )\n    ),\n    \"KMeans\": make_pipeline(\n        make_column_transformer(\n            (sklearn.preprocessing.OneHotEncoder(), ['island', 'sex']),\n            remainder = \"passthrough\"\n        ),\n        sklearn.cluster.KMeans()\n    )\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We will now setup a function which allows us to explore the model of our choice and change the basic parameters `n_clusters` for both and `linkage` for the agglomerative clustering. The function will then report the performance of the model by showing the allocation of species within each of the predicted clusters as well as show the calculated homogenity and completeness of these clusters. In the case of the agglomerative clustering, a dendrogram of the cluster will also be shown.",
      "metadata": {
        "cell_id": "00046-920a55c8-3f68-42c4-8a00-9bcdfffd18bb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00047-efeb54fe-051e-426c-86a2-e8658aac7691",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "aea8c609",
        "execution_millis": 0,
        "execution_start": 1616600037135,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "source": "def fit_penguin(m, n_clusters=4, linkage=\"ward\"):\n    m = models[m]\n    \n    model_type = m.steps[-1][0]\n    params = {}\n    params[model_type+\"__n_clusters\"] = n_clusters\n    \n    if (model_type == \"agglomerativeclustering\"):\n        params[model_type+\"__linkage\"] = linkage\n    \n    m.set_params(**params)\n    \n    \n    # Fit model\n    X = penguin.drop([\"species\"], axis=1)\n    m_fit = m.fit(X)\n    \n    # Calc scores\n    homogenity = sklearn.metrics.homogeneity_score(penguin.species, get_labels(m_fit))\n    completeness = sklearn.metrics.completeness_score(penguin.species, get_labels(m_fit))\n    \n    # Count species in clusters\n    pred = pd.DataFrame(penguin.species)\n    pred[\"cluster\"] = get_labels(m_fit)\n    \n    clusters = pred.value_counts(sort=False).rename(\"n\").to_frame().reset_index()\n    \n    # Plot\n    plt.figure(figsize=(12,6))\n    \n    plt.subplot(121)    \n    ax = sns.barplot(x=\"cluster\", y =\"n\", hue=\"species\", data = clusters, palette = sns.color_palette(\"Set2\"))\n    ax.set_title(\"Homogenity: {:.3f},   Completeness: {:.3f}\".format(homogenity, completeness))\n    \n    \n    if (model_type == \"agglomerativeclustering\"):\n        plt.subplot(122)\n        plot_dendrogram(m, no_labels = True)\n    \n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "fit_penguin('AgglomerativeClustering', n_clusters=4, linkage='ward')",
      "metadata": {
        "tags": [],
        "cell_id": "00056-b05bbf76-f0f8-4250-8eea-d8a2f74b8d34",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "906172a4",
        "execution_start": 1616600069622,
        "execution_millis": 450,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "fit_penguin('KMeans', n_clusters=4, linkage='ward')",
      "metadata": {
        "tags": [],
        "cell_id": "00057-f6b90c9c-fbdc-4ad8-a341-b9954ebcf7b5",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "bea7a8ae",
        "execution_start": 1616600078551,
        "execution_millis": 306,
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 13\n\nExperimenting with the given parameter choices, which approach (if any) appears to do the best job of correctly identifying the three species?",
      "metadata": {
        "cell_id": "00048-49de0fab-3f10-4b4d-bab5-ea7e36daea24",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00059-85ac4c65-2904-4216-b572-e87f70556007",
        "output_cleared": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00049-6d1ba409-7688-41df-b917-59550e8989cd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 14\n\nWe have previously seen that certain machine learning methods are sensitive to the scaling of the features, do you believe this is likely to be the case here, explain why or why not.",
      "metadata": {
        "cell_id": "00050-69c22b58-e905-4145-9346-a9cb25347071",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "cell_id": "00051-04fb058f-3d36-4bc2-8839-3fa08b35e30b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n### &diams; Exercise 15\n\nAdd two new models to the `models` dictionary that implements scaling of the numeric features as part of the pipeline (e.g. use `sklearn.preprocessing.StandardScaler`), name these `KMeans_Scaled` respectively.\n\nDoes this improve the performance of the clustering? Which approach (if any) now appears to be best?",
      "metadata": {
        "cell_id": "00052-c6a64ab5-05f6-4de8-8d80-b2f22e8c9b73",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00053-983341df-62eb-4284-9453-873e88b02adf",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## 4. Competing the worksheet\n\nAt this point you have hopefully been able to complete all the preceeding exercises. Now \nis a good time to check the reproducibility of this document by restarting the notebook's\nkernel and rerunning all cells in order.\n\nOnce that is done and you are happy with everything, you can then run the following cell \nto generate your PDF and turn it in on gradescope under the `mlp-week10` assignment.",
      "metadata": {
        "tags": [],
        "cell_id": "00054-9114c456-1051-4834-92e1-78a99be58405",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 7846,
        "output_cleared": true,
        "source_hash": "81b20c43",
        "tags": [],
        "cell_id": "00055-25c38e60-a25f-4861-b74c-c14db19a636d",
        "execution_start": 1616599579245,
        "deepnote_cell_type": "code"
      },
      "source": "!jupyter nbconvert --to pdf mlp-week10.ipynb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6ead8873-89c2-4b59-aeb0-b279f1329db6' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "deepnote_notebook_id": "a5c59a53-c3cd-4d1f-a370-db5159bab89b",
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": []
  }
}