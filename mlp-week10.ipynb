{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python - Workshop 10\n",
    "\n",
    "In this week's workshop we will be exploring several methods for unsupervised learning, specifically some of sklearn's tools for clustering data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "\n",
    "## 1.1 Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ipywidgets \n",
    "from ipywidgets import interact\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.manifold\n",
    "import sklearn.preprocessing\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import scipy.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Helper Functions\n",
    "\n",
    "For this lab we will make use of a the `plot_dendrogram` function provided in sklearn's clustering documentation. This function takes an `AgglomerativeClustering` model and plots the hierarchical merging of clusters as a dendrogram, more on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(m):\n",
    "    if hasattr(m, 'labels_'):\n",
    "        return(m.labels_)\n",
    "    else:\n",
    "        return(m.steps[-1][1].labels_)\n",
    "    \n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    # from sklearn documentaion:\n",
    "    # https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
    "    \n",
    "    # create the counts of samples under each node\n",
    "    \n",
    "    if (type(model) == sklearn.pipeline.Pipeline):\n",
    "        model = model.steps[-1][1]\n",
    "    \n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    labels = get_labels(model)\n",
    "    n_samples = len(labels)\n",
    "    \n",
    "    \n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    Z = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    palette = [mpl.colors.rgb2hex(c) for c in sns.color_palette(n_colors = model.n_clusters_)]\n",
    "    \n",
    "    link_cols = {}\n",
    "    for i, i12 in enumerate(Z[:,:2].astype(int)):\n",
    "        c1, c2 = (link_cols[x] if x > len(Z) else palette[labels[x]] for x in i12)\n",
    "        link_cols[i+1+len(Z)] = c1 if c1 == c2 else \"#808080\"\n",
    "    \n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(Z, link_color_func=lambda x: link_cols[x], **kwargs)\n",
    "    \n",
    "    \n",
    "    if (model.n_clusters_ == 1):\n",
    "        threshold = np.max(model.distances_)\n",
    "    elif (model.distance_threshold is None):\n",
    "        i = model.n_clusters_-1\n",
    "        threshold = (model.distances_[-i] + model.distances_[-(i+1)])/2\n",
    "    else:\n",
    "        threshold = model.distance_threshold\n",
    "    \n",
    "    plt.axhline(y=threshold, color=\"0.25\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data\n",
    "\n",
    "We will look at several toy data examples to start to get a better understanding of sklearn's clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"d1\": pd.read_csv(\"d1.csv\"),\n",
    "    \"d2\": pd.read_csv(\"d2.csv\"),\n",
    "    \"d3\": pd.read_csv(\"d3.csv\"),\n",
    "    \"d4\": pd.read_csv(\"d4.csv\"),\n",
    "    \"d5\": pd.read_csv(\"d5.csv\"),\n",
    "    \"d6\": pd.read_csv(\"d6.csv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all six of the data sets we have observations in 2 dimensions with 1, 2, or 3 classes. Our goal will be to attempt to recover these labels as best we are able without providing the true labels to the underlying algorithm fitting our model(s), which is why these methods are called as unsupervised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10.5,7))\n",
    "\n",
    "for i, d in zip(range(len(data)), data):\n",
    "    plt.subplot(231+i)\n",
    "    ax = sns.scatterplot(x='x', y='y', hue='label', data=data[d], legend=False)\n",
    "    ax.set_title(d)\n",
    "    if (i < 3): ax.set_xlabel(\"\")\n",
    "    if (i % 3 != 0): ax.set_ylabel(\"\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. K-Means\n",
    "\n",
    "We will begin by fitting k-means models to the data sets while adjusting several of the key model parameters. k-means models are straight forward to fit and use a the same sklearn interface that we are used to, they only differ in that they only require a model / data matrix `X` and do not require or use the outcome vector `y`. For the sake of compatibility with the supervised models in sklearn the `fit` method of these models still has an argument `y`, but this is argument is neither required nor used. \n",
    "\n",
    "Since we are often only interested in the result of the clustering, and not the underlying model, k-means and the other clustering models offer a `fit_predict` method which both fits the model and then predicts cluster labels as a single step for a data matrix `X`. This is similar in spirit to the `fit_transform` method that is present with the preprocessing transformer like `StandardScaler` and `OneHotEncoder`.\n",
    "\n",
    "Below we implement a simple interactive tool for exploring all six data sets with the k-means algorithm. We have provided widgets that allow you to alter the number of clusters and the method for initializing the cluster centroids (for more on the k-means++ method see this [article](https://en.wikipedia.org/wiki/K-means%2B%2B#Improved_initialization_algorithm)) and the random state value to seed the random number generator before fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    d=[\"d1\", \"d2\"], \n",
    "    n_clusters=(1,5), \n",
    "    init=['k-means++', 'random'], \n",
    "    random_state='1234'\n",
    ")\n",
    "\n",
    "def fit_kmeans(d, n_clusters=2, init='random', random_state='1234'):\n",
    "    # Convert from string to int\n",
    "    random_state = int(random_state)\n",
    "    \n",
    "    df = data[d]\n",
    "    X = df.drop(['label'], axis=1)\n",
    "\n",
    "    # Fit and predict\n",
    "    df['cluster'] = sklearn.cluster.KMeans(\n",
    "        n_clusters=n_clusters, init=init, random_state=random_state\n",
    "    ).fit_predict(X)\n",
    "    df = df.astype({'cluster': 'category'})\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(7,7))\n",
    "    ax = sns.scatterplot(x='x', y='y', hue='cluster', style='label', data=df)\n",
    "    ax.set_title(d)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 1\n",
    "\n",
    "For data set `d1` what happens when you change the value of `random_state` with `n_clusters=2`? *Hint* - keep trying until something interesting happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 2\n",
    "\n",
    "For data set `d1` set `n_clusters=3` do the clusters change as you vary `init` and `random_state`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 3\n",
    "\n",
    "Explore the different data sets with different values of `n_cluster`, describe the nature of the boundaries produced by a k-means clustering. *Hint* - don't just consider the binary (`k=2`) case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 4\n",
    "\n",
    "Explain why we can or cannot construct a confusion matrix using the predictions from the k-means model (for the purposes of model scoring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In all of the provided data sets the true number of clusters is provided and is also fairly obvious just by visual inspection of the data. However, with real world data this value is unlikely to be known ahead of time. A simple way of potentially determining this value is to fit a series of k-means models with different values of `k` an then comparing the resulting within-cluster sum-of-squares (or what sklearn calls the inertia).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 5\n",
    "\n",
    "In general, would you expect the within-cluster sum-of-squares (inertia) to increase, decrease or stay the same as `k` is increased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Below we have reused the code from our previous interactive plot but now we include a second plot that shows the inertia as a function of `n_cluster`. For each of the data sets play with different values of `n_clusters` and try to develop some intuition on how the inertia relates to the model's fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    d=data.keys(), \n",
    "    n_clusters=(1,5)\n",
    ")\n",
    "\n",
    "def fit_kmeans(d, n_clusters=2):\n",
    "    # Read the data, create the model matrix, and fit and predict\n",
    "    df = data[d]\n",
    "    X = df.drop(['label'], axis=1)\n",
    "    \n",
    "    models = [ sklearn.cluster.KMeans(n_clusters=n).fit(X) for n in range(1,6)]\n",
    "    scores = [m.inertia_ for m in models]\n",
    "    \n",
    "    df['cluster'] = models[n_clusters-1].predict(X)\n",
    "    df = df.astype({'cluster': 'category'})\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    sns.scatterplot(x='x', y='y', hue='cluster', style='label', data=df)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(np.arange(1,6), scores, 'o-')\n",
    "    plt.xlabel(\"n_clusters\")\n",
    "    plt.ylabel(\"inertia\")\n",
    "    plt.xticks(range(1,6))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 6\n",
    "\n",
    "Based on these plots, for which data sets does k-means appear to be doing a good job - i.e. it fits well and the correct number of clusters can be inferred from the data. For which data sets does it not do a good job?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 7\n",
    "\n",
    "Describe the features of the inertia plots that suggest whether a k-means model is fitting well and how this relates to determining $n_{clusters}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hierarchical Clustering\n",
    "\n",
    "While there is a large number of different methods for hierarchical clustering, the methods implemented in sklearn under the `AgglomerativeClustering` model use a bottom up approach by starting each observation as its own cluster and then merging existing clusters to create new larger clusters. The rule(s) used for merging these clusters are determined by the `linkage` argument which takes one of the following values:\n",
    "\n",
    "* `\"ward\"` - minimizes the sum of squared differences within all clusters.\n",
    "* `\"complete\"` - minimizes the maximum distance between observations of pairs of clusters.\n",
    "* `\"average\"` - minimizes the average of the distances between all observations of pairs of clusters.\n",
    "* `\"single\"` - minimizes the distance between the closest observations of pairs of clusters.\n",
    "\n",
    "The interactive widgets below will let you play with these different approaches. We have also provided the option of specifying `n_clusters` or `distance_threshold` - the former is ignored if the latter is not `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    d=data.keys(),\n",
    "    n_clusters=(1,5),\n",
    "    distance_threshold=\"\",\n",
    "    linkage = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    ")\n",
    "\n",
    "def fit_aggclust(d, n_clusters, distance_threshold, linkage):\n",
    "    #distance_threshold = float(distance_threshold)\n",
    "    \n",
    "    try:\n",
    "        distance_threshold = float(distance_threshold)\n",
    "    except:\n",
    "        distance_threshold = None\n",
    "    \n",
    "    if (distance_threshold is not None):\n",
    "        n_clusters = None\n",
    "    \n",
    "    d = data[d]\n",
    "    X = d.drop(['label'], axis=1)\n",
    "    \n",
    "    # Fit model\n",
    "    m = sklearn.cluster.AgglomerativeClustering(\n",
    "        distance_threshold=distance_threshold, n_clusters=n_clusters, \n",
    "        linkage=linkage, compute_full_tree = True, compute_distances = True\n",
    "    ).fit(X)\n",
    "    \n",
    "    d['cluster'] = m.labels_\n",
    "    d['cluster'] = d['cluster'].astype('category')\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    sns.scatterplot(x='x', y='y', hue='cluster', style='label', data=d)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plot_dendrogram(m, show_leaf_counts=False, no_labels=True)\n",
    "\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 8\n",
    "\n",
    "How does the `distance_threshold` argument relate to the number of clusters predicted by the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 9\n",
    "\n",
    "For `d1` and `d2` which of the linkage method(s) work best for identifying the correct number of clusters present?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 10\n",
    "\n",
    "What about for `d3` and `d4`? Which of the linkage method(s) work best for identifying the correct number of clusters present?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 11\n",
    "\n",
    "Finally, what about for `d5` and `d6`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 12\n",
    "\n",
    "Based on what you found with Exercises 9-11 can you come up with a reasonable heuristic for choosing which linkage function to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Clustering Animals\n",
    "\n",
    "We will wrap this workshop up by looking at a slightly more complicated example. These data come from the `palmerpenguin` R [package](https://github.com/allisonhorst/palmerpenguins) by Allison Horst and represent data collected on penguin species occuring around Palmer Station in Antarctica by Dr. Kristen Gorman. The data contain various measurement details of the sample penguin's physical characteristics as well as the specific island they were located on. The data provided in `palmerpenguin.csv` reflect a lightly modified version of the data available in the package (missing values and the `year` variable have been removed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin = pd.read_csv(\"palmerpenguins.csv\")\n",
    "penguin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to now use the clustering methods we've just learned and see how well we are able to distinguish between the different species of penguin using an *unsupervised* modelling approach. Specifically, in this case we already know the species, but it is conceivable that a similar project could have been undertaken where the question of interest is how many species are present amoung the sampled individuals.\n",
    "\n",
    "Below we set up a dictionary containing two pipelines, one for agglomerative clustering and the other for k means clustering. These pipelines are configured so that they take care of the dummy coding for the categorical variables and the basic configuration of the clustering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"AgglomerativeClustering\": make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (sklearn.preprocessing.OneHotEncoder(), ['island', 'sex']),\n",
    "            remainder = \"passthrough\"\n",
    "        ),\n",
    "        sklearn.cluster.AgglomerativeClustering(\n",
    "            compute_full_tree = True, compute_distances = True\n",
    "        )\n",
    "    ),\n",
    "    \"KMeans\": make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (sklearn.preprocessing.OneHotEncoder(), ['island', 'sex']),\n",
    "            remainder = \"passthrough\"\n",
    "        ),\n",
    "        sklearn.cluster.KMeans()\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now setup a function which allows us to interactively select the model of our choice and change the basic parameters `n_clusters` for both and `linkage` for the agglomerative clustering. The function will then report the performance of the model by showing the allocation of species within each of the predicted clusters as well as show the calculated homogenity and completeness of these clusters. In the case of the agglomerative clustering, a dendrogram of the cluster will also be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    m = models.keys(),\n",
    "    n_clusters=(1,8),\n",
    "    linkage = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    ")\n",
    "\n",
    "def fit_penguin(m, n_clusters=4, linkage=\"ward\"):\n",
    "    m = models[m]\n",
    "    \n",
    "    model_type = m.steps[-1][0]\n",
    "    params = {}\n",
    "    params[model_type+\"__n_clusters\"] = n_clusters\n",
    "    \n",
    "    if (model_type == \"agglomerativeclustering\"):\n",
    "        params[model_type+\"__linkage\"] = linkage\n",
    "    \n",
    "    m.set_params(**params)\n",
    "    \n",
    "    \n",
    "    # Fit model\n",
    "    X = penguin.drop([\"species\"], axis=1)\n",
    "    m_fit = m.fit(X)\n",
    "    \n",
    "    # Calc scores\n",
    "    homogenity = sklearn.metrics.homogeneity_score(penguin.species, get_labels(m_fit))\n",
    "    completeness = sklearn.metrics.completeness_score(penguin.species, get_labels(m_fit))\n",
    "    \n",
    "    # Count species in clusters\n",
    "    pred = pd.DataFrame(penguin.species)\n",
    "    pred[\"cluster\"] = get_labels(m_fit)\n",
    "    \n",
    "    clusters = pred.value_counts(sort=False).rename(\"n\").to_frame().reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    plt.subplot(121)    \n",
    "    ax = sns.barplot(x=\"cluster\", y =\"n\", hue=\"species\", data = clusters, palette = sns.color_palette(\"Set2\"))\n",
    "    ax.set_title(\"Homogenity: {:.3f},   Completeness: {:.3f}\".format(homogenity, completeness))\n",
    "    \n",
    "    \n",
    "    if (model_type == \"agglomerativeclustering\"):\n",
    "        plt.subplot(122)\n",
    "        plot_dendrogram(m, no_labels = True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 13\n",
    "\n",
    "Experimenting with the given parameter choices, which approach (if any) appears to do the best job of correctly identifying the three species?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 14\n",
    "\n",
    "We have previously seen that certain machine learning methods are sensitive to the scaling of the features, do you believe this is likely to be the case here, explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### &diams; Exercise 15\n",
    "\n",
    "Add two new models to the `models` dictionary that implements scaling of the numeric features as part of the pipeline (e.g. use `sklearn.preprocessing.StandardScaler`), name these `KMeans_Scaled` respectively.\n",
    "\n",
    "Does this improve the performance of the clustering? Which approach (if any) now appears to be best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00086-972a2e5e-b9de-4c9e-9231-469e6d139e3c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Competing the worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the preceeding exercises. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF and turn it in on gradescope under the `mlp-week10` assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00087-73e67ae4-48f2-42d5-b43e-47f2799ed10e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11110,
    "execution_start": 1615986298562,
    "output_cleared": true,
    "source_hash": "a80049a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp-week10.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
